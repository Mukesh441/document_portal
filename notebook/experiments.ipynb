{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3abd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9526d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2dc27317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e27696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9165c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b39a0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"What is the capital of France?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed9cedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26f43a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0530747",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector=embedding_model.embed_query(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57653c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04257791489362717, -0.047810737043619156, -0.02702580951154232, -0.035097863525152206, 0.05324113741517067, 0.0018493696115911007, 0.004823467694222927, -0.022051338106393814, 0.0009697225177660584, 0.07324519753456116, -0.014812891371548176, 0.003644853364676237, -0.00034491211408749223, 0.028128888458013535, 0.025020018219947815, -0.04156218096613884, 0.005471833515912294, 0.02652869001030922, 0.043672770261764526, -0.014782802201807499, 0.013127882033586502, 0.007567551918327808, -0.03469207510352135, 0.023462051525712013, 0.020962651818990707, -0.05559537559747696, 0.00859882216900587, -0.04824773594737053, -0.012368501164019108, -0.001532181748189032, -0.07255345582962036, 0.04269229248166084, 0.00527808116748929, -0.015593086369335651, 0.02645784802734852, -0.0531519278883934, -0.0004922056687064469, 0.016876710578799248, -0.00814562477171421, 0.04225021228194237, -0.015036126598715782, -0.003916633781045675, -0.04687097668647766, 0.015219401568174362, -0.009408559650182724, -0.01825689896941185, -0.01993844285607338, 0.0748581662774086, 0.019254358485341072, -0.0052777305245399475, 0.012134595774114132, -0.010617725551128387, 0.054592013359069824, 0.020773116499185562, 0.013602628372609615, -0.06971295922994614, 0.008997276425361633, -0.014119189232587814, -0.0046128276735544205, 0.0210944302380085, 0.0329081267118454, -0.030065499246120453, 0.00447330716997385, 0.0407467782497406, 0.01971225067973137, -0.055106159299612045, 0.03556030988693237, 0.013304406777024269, 0.08534496277570724, 0.00158949033357203, -0.004593267105519772, -0.042083490639925, 0.0713706836104393, 0.005657872185111046, 0.030689362436532974, -0.0817095935344696, -0.02232016995549202, 0.06454379111528397, 0.01360271591693163, -0.04246792569756508, -0.00804409384727478, -0.05186443775892258, -0.07956809550523758, -0.02714453637599945, -0.05819103121757507, 0.014083041809499264, -0.05621412768959999, -0.018843192607164383, -0.05411393195390701, 0.04985203966498375, -0.025272108614444733, 0.006586411036550999, 0.07480309158563614, -0.040020957589149475, -0.028987407684326172, 0.06478418409824371, -0.014494857750833035, -0.011784432455897331, 0.07746846228837967, -0.003581888973712921, 0.01965465024113655, -0.021930739283561707, -0.07060352712869644, 0.05417868122458458, 0.03164941444993019, -0.00866173766553402, -0.01214554999023676, 0.058456458151340485, 0.006299072410911322, 0.06679613888263702, -0.0667213425040245, -0.04552998021245003, 0.031006908044219017, 0.06095348298549652, 0.01496248971670866, -0.05309277027845383, -0.03518706187605858, 0.017601503059267998, 0.03776673972606659, 0.024583736434578896, 0.035969726741313934, -0.018826857209205627, 0.044718313962221146, -0.045678626745939255, 0.034775298088788986, -0.01982470043003559, -0.04870112985372543, 0.032311610877513885, -0.011565012857317924, 0.021644998341798782, 0.015746233984827995, -0.04829183965921402, -0.014721618965268135, -0.002236217027530074, 0.014409353025257587, 0.030544603243470192, 0.061715949326753616, -0.018756143748760223, 0.04099138453602791, 0.006017507519572973, 0.029441386461257935, 0.0671597346663475, 0.0025576308835297823, 0.028602181002497673, -0.018605684861540794, 0.05013180524110794, -0.0542902871966362, -0.03664889186620712, 0.040750712156295776, -0.044509004801511765, -0.07576945424079895, 0.0016938719199970365, -0.04662192612886429, -0.02760942094027996, 0.0392257384955883, 0.010924643836915493, -0.0143031757324934, 0.048677846789360046, 0.01945548690855503, -0.0165046788752079, 0.05750593915581703, 0.017886588349938393, 0.016367116943001747, 0.006838107947260141, -0.0027852400671690702, -0.028113022446632385, 0.03293533995747566, -0.008162261918187141, 0.016503559425473213, -0.0008890617173165083, -0.011331772431731224, 0.01560590323060751, -0.002747876103967428, -0.023822380229830742, 0.008317090570926666, -0.033890966325998306, 0.011247474700212479, -0.02326224185526371, -0.014204729348421097, -0.031394872814416885, 0.006071117240935564, -0.03975209593772888, 0.006350292824208736, 0.03685023635625839, 0.011773370206356049, -0.043271128088235855, 0.022107595577836037, -0.03222227841615677, 0.004633280448615551, 0.01861286163330078, -0.0461781807243824, -0.013411097228527069, -0.014593689702451229, -0.017189396545290947, -0.037615299224853516, -0.01175762340426445, 0.029062220826745033, -0.017437946051359177, 0.0454525351524353, -0.05209745094180107, -0.029428549110889435, 0.03321756049990654, 0.02435440942645073, -0.0358048640191555, 0.022126683965325356, -0.00966416671872139, 0.08779031038284302, -0.03464784100651741, -0.043734170496463776, 0.05736429989337921, -0.014013061299920082, 0.006000404711812735, -0.02421995811164379, 0.016350895166397095, 0.044808026403188705, -0.025562455877661705, 0.07690118998289108, 0.010970678180456161, 0.04207293689250946, -0.022715603932738304, -0.04467586800456047, -0.003055560402572155, -0.023600663989782333, -0.013115497305989265, 0.04398322477936745, 0.023938285186886787, -0.011075073853135109, -0.03778417780995369, 0.00021477344853337854, -0.010648282244801521, -0.05003296211361885, 0.07658620178699493, 0.03315397724509239, -0.02210102789103985, 0.05767066404223442, 0.0006767681916244328, -0.003242972306907177, 0.020267846062779427, 0.0006321387481875718, 0.0009972446132451296, -0.02512633241713047, -0.03888467326760292, 0.04662318155169487, 0.023061562329530716, -0.04041805863380432, -0.038651108741760254, -0.027900371700525284, 0.024071509018540382, 0.05060867220163345, 0.05143703520298004, -0.002084001898765564, 0.0024474747478961945, 0.032136015594005585, 0.019393740221858025, -0.07943607866764069, 0.02427038736641407, -0.06520397216081619, 0.031165381893515587, -0.035928234457969666, 0.03409767523407936, 0.030861197039484978, 0.03538651764392853, 0.042785514146089554, -0.03435273468494415, -0.015017978847026825, -0.03510842099785805, 0.006179507356137037, -0.05111760273575783, 0.012249006889760494, 0.005350593011826277, 0.03790914639830589, -0.07914953678846359, 0.017817426472902298, 0.0395965501666069, 0.036239635199308395, 0.012556263245642185, -0.050666097551584244, 0.057882554829120636, 0.06475129723548889, -0.0725456178188324, 0.03519894927740097, 0.030570028349757195, -0.006167229264974594, -0.011796296574175358, -0.008290175348520279, -0.017933540046215057, -0.04209362342953682, -0.012064228765666485, 0.00880552176386118, -0.05999321490526199, -0.03464368358254433, -0.08186905086040497, 0.0355706512928009, -0.04216013848781586, -0.10391668230295181, 0.0038084639236330986, -0.030506454408168793, 0.0399044007062912, 0.03539436310529709, -0.01851348765194416, -0.030506830662488937, 0.01296560000628233, 0.02962312288582325, -0.05594923719763756, 0.026137271896004677, 0.027979889884591103, -0.032783620059490204, -0.05861344188451767, 0.029305243864655495, 0.025631634518504143, 0.046621762216091156, -0.01578911766409874, -0.05855393409729004, -0.033567510545253754, -0.022099914029240608, 0.07099347561597824, -0.015482406131923199, -0.02221505530178547, 0.020550455898046494, 0.03193112462759018, 0.009649628773331642, 0.08112385869026184, 0.024486811831593513, -0.020967384800314903, -0.001703555346466601, 0.003984694369137287, 0.01214590948075056, 0.02251713164150715, 0.011797886341810226, -0.014598767273128033, -0.027774184942245483, 0.035769712179899216, -0.04636594280600548, 0.006575292441993952, 0.01091056503355503, 0.06001884862780571, -0.03792550787329674, 0.025552064180374146, -0.052944615483284, -0.00331985205411911, 0.04723644629120827, 0.04916655644774437, -0.012118092738091946, -0.015976974740624428, -0.023780548945069313, -0.020015906542539597, -0.016765648499131203, 0.017619017511606216, 0.09507094323635101, 0.02456720732152462, -0.00030371572938747704, 0.07726727426052094, 0.009286770597100258, 0.04579121246933937, 0.0006386045133695006, -0.0203663669526577, 0.05929982289671898, -0.009882405400276184, 0.017564021050930023, -0.05515163019299507, 0.016189292073249817, 0.028766026720404625, -0.03514404594898224, -0.04935841262340546, -0.02447367087006569, -0.02685004286468029, -0.009837007150053978, 9.710079029900953e-05, 0.019255781546235085, 0.016495689749717712, 0.019392136484384537, 0.014026164077222347, 0.044185783714056015, -0.0408172607421875, 0.058831170201301575, 0.033452264964580536, -0.07100701332092285, -0.01207928266376257, 0.017947617918252945, 0.026635218411684036, -0.042489077895879745, -0.025743063539266586, 0.04628248140215874, 0.02357054501771927, 0.012330091558396816, -0.019114600494503975, 0.04284172132611275, 0.020867055281996727, -0.020985012874007225, 0.049807094037532806, -0.06530244648456573, 0.06784137338399887, 0.06132720038294792, -0.025619087740778923, -0.01975896954536438, -0.028557993471622467, -0.0033664510119706392, -0.022265056148171425, 0.0277670007199049, 0.0376681424677372, -0.01844070851802826, -0.0666554793715477, -0.03576522693037987, -0.006980367470532656, -0.009456862695515156, 0.007616228424012661, -0.002117250580340624, -0.000508638215251267, -0.058485377579927444, 0.02196097932755947, 0.011461308225989342, -0.019124537706375122, 0.017011091113090515, -0.03894034028053284, -0.024677084758877754, -0.009032917208969593, 0.019948668777942657, -0.020086267963051796, 0.0046862466260790825, 0.0658910721540451, -0.021722840145230293, -0.0028560664504766464, 0.011400393210351467, -0.014010073617100716, -0.05337538197636604, -0.07439275830984116, 0.0002861053217202425, 0.04116436839103699, 0.03818747028708458, -0.012812647968530655, 0.05230933055281639, 0.018940966576337814, -0.048808012157678604, -0.06779120117425919, -0.01911095529794693, -0.028975164517760277, 0.004133264068514109, 0.01923450082540512, -0.020669246092438698, -0.003695683553814888, 0.009470553137362003, -0.041596852242946625, 0.045980364084243774, 0.029285280033946037, -0.07706877589225769, 0.006656208075582981, -0.005012008361518383, -0.017416084185242653, 0.007063459139317274, -0.05976015701889992, 0.0185412485152483, -0.0273988489061594, 0.005803277250379324, -0.041785452514886856, -0.0908936932682991, -0.006833197548985481, -0.014357824809849262, 0.08537802845239639, -0.05669703334569931, 0.06276111304759979, 0.0009012018563225865, -0.007769424468278885, -0.02144297957420349, -0.11318439245223999, 0.07541830092668533, 0.023151574656367302, 0.005866213236004114, -0.004821085371077061, -0.010738340206444263, 0.02703235298395157, 0.025599531829357147, 0.007024332880973816, -0.032584112137556076, -0.04185543581843376, -0.024687552824616432, -0.02051878534257412, -0.04894666746258736, 0.03619479760527611, -0.04281015321612358, 0.014844655990600586, 0.010251615196466446, 0.023320626467466354, 0.01871577650308609, 0.03378577157855034, -0.025527965277433395, 0.044417854398489, -0.02600604109466076, -0.0119530213996768, -0.055049628019332886, 0.022267477586865425, -0.010707407258450985, -0.026371469721198082, -0.009768350049853325, -0.01535708550363779, -0.019579773768782616, -0.021421188488602638, 0.006235864479094744, 0.0314040407538414, 0.031010085716843605, 0.004555661231279373, -0.031087100505828857, -0.014605932869017124, -0.003368874778971076, -0.028359955176711082, 0.057969674468040466, -0.09270115196704865, -0.009005305357277393, 0.022297324612736702, -0.02122938074171543, -0.03563996031880379, 0.004106925334781408, 0.006597382482141256, 0.03264277055859566, 0.03904365748167038, 0.02032073214650154, 0.005939505994319916, 0.010560653172433376, -0.002243859926238656, 0.02130184881389141, -0.02763022854924202, 0.014897515065968037, -0.020667249336838722, -0.09472247958183289, -0.0004307603812776506, -0.02479407750070095, 0.028932971879839897, 0.025980545207858086, 0.057643499225378036, -0.0374685563147068, 0.0006257054628804326, -0.01778700202703476, 0.017848286777734756, -0.00700916163623333, -0.026394061744213104, 0.03204090893268585, -0.005773015785962343, -0.012837935239076614, 0.016041608527302742, 0.030365966260433197, -0.025132445618510246, 0.0380871519446373, 0.026565272361040115, 0.061460208147764206, 0.018262824043631554, -0.025590986013412476, -0.014801396057009697, -0.009373162873089314, -0.05065552145242691, -0.01938270404934883, 0.04338742420077324, -0.008704069070518017, 0.020551657304167747, 0.010078946128487587, -0.022332625463604927, 0.013624574989080429, -0.029078511521220207, -0.02031247317790985, 0.04527844116091728, 0.008548842743039131, -0.044655926525592804, -0.04993825778365135, -0.0005616651615127921, -0.0019093779847025871, 0.029475873336195946, 0.08492345362901688, 0.02737259492278099, -0.019202347844839096, -0.011180019937455654, 0.0613800473511219, 0.0009942551841959357, -0.005685679614543915, 0.010302840732038021, 0.0048909238539636135, 0.03689644858241081, 6.452776142396033e-05, -0.02497044950723648, 0.006457015406340361, -0.03664189949631691, -0.03386320546269417, -0.009320611134171486, 0.027527937665581703, -0.006957217585295439, -0.0016937933396548033, 0.02136092633008957, 0.008775141090154648, 0.031389400362968445, 0.05911479890346527, 0.10196645557880402, 0.03833167254924774, 0.059265561401844025, -0.034477267414331436, 0.01983230747282505, -0.028979182243347168, -0.0006717467331327498, 0.023720132187008858, 0.009949897415935993, -0.027036644518375397, -0.02788533642888069, -0.009638091549277306, -0.017497442662715912, 0.05965723469853401, -0.023421915248036385, -7.17904549674131e-05, -0.023910334333777428, 0.010167197324335575, 0.06294538080692291, -0.007855839096009731, 0.009582506492733955, -0.00771779241040349, 0.010753695853054523, 0.029988912865519524, -0.0410747155547142, 0.014009366743266582, -0.016409873962402344, -0.025754181668162346, -0.029275324195623398, 0.06891670823097229, 0.00874270685017109, 0.023196490481495857, -0.038228556513786316, 0.016259821131825447, -0.013566256500780582, 0.025782182812690735, -0.020683975890278816, 0.04356987029314041, 0.03843095898628235, 0.011759771965444088, 0.0006424587918445468, 0.06934863328933716, 0.004451766610145569, 0.05257083848118782, 0.05299004167318344, -0.027428876608610153, 0.007362625561654568, -0.027196450158953667, -0.03716901317238808, -0.05996355041861534, -0.018047159537672997, 0.010596984066069126, -0.019895626232028008, -0.02837696112692356, -0.02666792832314968, 0.005059539806097746, -0.00711184972897172, 0.028563370928168297, 0.10317031294107437, 0.04524599015712738, -0.09671919792890549, -0.05298631638288498, -0.02709105610847473, -0.03117411397397518, -0.010037784464657307, 0.0028370970394462347, -0.01989951729774475, 0.054531022906303406, 0.017425885424017906, -0.031392212957143784, -0.0492844320833683, 0.012391205877065659, 0.021212412044405937, -0.010904794558882713, -0.02079067938029766, 0.010785725899040699, 0.0042291851714253426, 0.007526397705078125, 0.018077723681926727, -0.030186735093593597, -0.06504229456186295, -0.0060129123739898205, 0.04470229893922806, -0.0001683917362242937, 0.014180039055645466, 0.02072112262248993, 0.001374627579934895, 0.03986472263932228, 0.02338232658803463, 0.0028986725956201553, 0.018854621797800064, 0.018174119293689728, 0.008018970489501953, -0.001832504291087389, 0.004592920653522015, -0.01788954995572567, 0.004617960192263126, -0.028025709092617035, 0.04705401882529259, 0.027661263942718506, 0.0065150074660778046, -0.030028309673070908, -0.05390259250998497, -0.0013852387201040983, -0.01643216609954834, -0.06801334023475647, 0.05069683864712715, 0.06540688872337341, 0.01848502829670906, 0.03506237268447876, 0.015128708444535732, -0.0009853191440925002, 0.018667006865143776, -0.02371547929942608, -0.03277934715151787, -0.04126175865530968, 0.0004407509695738554, -0.006913974415510893, 0.001071878825314343, 0.04121888801455498, 0.033742137253284454, 0.06251147389411926, 0.07336639612913132, 0.036337900906801224, -0.020660797134041786, -0.023938000202178955, 0.02654903009533882, -0.009692799299955368, -0.04504105821251869, 0.022294068709015846, 0.03183344379067421, -0.013615928590297699, 0.09686916321516037, 0.05850844457745552, 0.05051637440919876, 0.06556273996829987, -0.030124453827738762, -0.05741645395755768, 0.08546410501003265, -0.09430325776338577, -0.0328102707862854, -0.026789216324687004, 0.03717803210020065, 0.08715295791625977, 0.0001604699791641906, -0.009505724534392357, -0.01792062073945999, 0.008379129692912102, 0.0796549916267395, 0.033412281423807144, 0.04523250833153725, 0.002415842143818736, -0.08875352889299393, -0.02112312614917755, 0.034831877797842026, 0.008336428552865982, 0.03964807465672493, 0.022455615922808647, 0.010564827360212803, -0.02183358184993267, -0.03671906143426895, 0.016374699771404266, -0.07664699852466583, -0.04381590336561203, 0.008805062621831894, -0.029425987973809242, 0.035350218415260315, 0.03271523490548134, -0.008388573303818703, -0.021349893882870674, 0.01584000512957573, -0.014425228349864483, -0.001280902768485248, -0.015345528721809387, -0.05014083534479141, 0.004639973398298025, 0.00799761712551117, 0.01279925275593996, 0.01869218423962593, 0.050975508987903595, -0.004513971973210573]\n"
     ]
    }
   ],
   "source": [
    "print(doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968c0f6",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9dd60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "# for Multiple PDF files, you can below class\n",
    "# from langchain.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd317841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2e86776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4745c338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ae31cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.path.join(os.getcwd(), \"data\", \"sample.pdf\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd684362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=PyPDFLoader(file_path).load()\n",
    "#  For multiple PDF files, you can use the below class\n",
    "# loader=PyPDFDirectoryLoader(\"data/\").load()\n",
    "len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78660bdf",
   "metadata": {},
   "source": [
    "# Data Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9d43da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e9fef0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron∗ Louis Martin† Kevin Stone†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_split=text_splitter.split_documents(loader)\n",
    "doc_split[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50ee11dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_split[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335177b4",
   "metadata": {},
   "source": [
    "# Vector Storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc144ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(doc_split, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8779c06",
   "metadata": {},
   "source": [
    "# This is a Retrieval proceesss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1fbaf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='09a43858-3e69-45dd-8acd-fb5a53b4df2c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='4b8a1445-d24d-4faf-bccb-3f56288570ae', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='8335f5c3-c214-4723-86d2-a090f9062661', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66'),\n",
       " Document(id='c890dfe8-e072-461f-a614-ca7a766db265', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 6, 'page_label': '7'}, page_content='models internally. For these models, we always pick the best score between our evaluation framework and\\nany publicly reported results.\\nIn Table 3, we summarize the overall performance across a suite of popular benchmarks. Note that safety\\nbenchmarks are shared in Section 4.1. The benchmarks are grouped into the categories listed below. The\\nresults for all the individual benchmarks are available in Section A.2.2.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc=vectorstore.similarity_search(\"llama2 finetuning benchmark experiments.\")\n",
    "relevant_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc1780f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7946cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_doc[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85e3eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61cde212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='4b8a1445-d24d-4faf-bccb-3f56288570ae', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 73, 'page_label': '74'}, page_content='Llama 2\\n7B 0.28 0.25 0.29 0.50 0.36 0.37 0.21 0.34 0.32 0.50 0.28 0.19 0.26 0.32 0.44 0.51 0.30 0.2513B 0.24 0.25 0.35 0.50 0.41 0.36 0.24 0.39 0.35 0.48 0.31 0.18 0.27 0.34 0.46 0.66 0.35 0.2834B 0.27 0.24 0.33 0.56 0.41 0.36 0.26 0.32 0.36 0.53 0.33 0.07 0.26 0.30 0.45 0.56 0.26 0.3570B 0.31 0.29 0.35 0.51 0.41 0.45 0.27 0.34 0.40 0.52 0.36 0.12 0.28 0.31 0.45 0.65 0.33 0.20\\nFine-tuned'),\n",
       " Document(id='09a43858-3e69-45dd-8acd-fb5a53b4df2c', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 7, 'page_label': '8'}, page_content='13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9\\n33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7\\n65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6\\nLlama 2\\n7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3\\n13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1\\n34B 27.8 69.9 58.7 68.0 24.2 62.6 44.1 43.4\\n70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2\\nTable 3: Overall performance on grouped academic benchmarks compared to open-source base models.'),\n",
       " Document(id='fc5405a3-ac0b-40c9-9c36-4480e0a4d737', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 43, 'page_label': '44'}, page_content='Ba. Large language models are human-level prompt engineers. InThe Eleventh International Conference on\\nLearning Representations, 2022.\\n44'),\n",
       " Document(id='66bbaba4-55eb-409a-8353-eeb0019e8c26', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='small delta (-0.9) between the \"clean\" subset performance and the sampling mean. No other dataset (for any\\nchoice ofL) appears to have benefitted from dataset contamination, and we omit results from these datasets\\nfor conciseness.\\n76'),\n",
       " Document(id='8335f5c3-c214-4723-86d2-a090f9062661', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\MSE226\\\\Desktop\\\\LLM_OPS_PRAC\\\\DOCUMENT_PORTAL\\\\notebook\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 70, 'page_label': '71'}, page_content='65B 14.27 31.59 21.90 14.89 23.51 22.27 17.16 18.91 28.40 19.32 28.71 22.00 20.03\\nLlama 2\\n7B 16.53 31.15 22.63 15.74 26.87 19.95 15.79 19.55 25.03 18.92 21.53 22.34 20.20\\n13B 21.29 37.25 22.81 17.77 32.65 24.13 21.05 20.19 35.40 27.69 26.99 28.26 23.84\\n34B 16.76 29.63 23.36 14.38 27.43 19.49 18.54 17.31 26.38 18.73 22.78 21.66 19.04\\n70B 21.29 32.90 25.91 16.92 30.60 21.35 16.93 21.47 30.42 20.12 31.05 28.43 22.35\\nFine-tuned\\nChatGPT 0.23 0.22 0.18 0 0.19 0 0.46 0 0.13 0 0.47 0 0.66')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"llama2 finetuning benchmark experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7f591eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Question: {question}\n",
    "        \n",
    "        Context: {context}     \n",
    "\n",
    "        Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "44640b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "212a001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"question\" ,\"context\" ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1221e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "02921649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "900ed68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "48f6c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    { \"question\" : RunnablePassthrough() , \"context\" : retriever | format_docs  }                                 \n",
    "    | prompt \n",
    "    | llm \n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "260e6fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, so I have this question about the key findings from the llama2 finetuning benchmark experiments. The context provided includes some tables and text, but it\\'s a bit confusing. Let me try to parse through it.\\n\\nFirst, I see a table with numbers labeled as 7B, 13B, etc., which I assume are model sizes in billions. Then there are several columns with numbers, possibly performance metrics. It mentions \"Overall performance on grouped academic benchmarks compared to open-source base models.\" So it looks like the Llama2 models of different sizes are being compared on various benchmarks.\\n\\nThe context also talks about \"small delta (-0.9)\" between clean subsets and sampling means, suggesting that the performance difference isn\\'t huge. It mentions that other datasets didn\\'t benefit from contamination, so those results are omitted. Then there\\'s another table with more numbers, possibly more detailed metrics.\\n\\nThere\\'s some text about training hyperparameters: AdamW optimizer with specific beta values, learning rate schedule, weight decay, and gradient clipping. This tells me about the training setup but not the results.\\n\\nAnother part mentions ablation experiments and selecting the best scores between internal and public results. It refers to safety benchmarks in another section and says individual results are available elsewhere.\\n\\nPutting it all together, the key findings would likely focus on how different model sizes (7B, 13B, etc.) perform across these benchmarks. The fact that the clean subset performance is only slightly different suggests that maybe the models are quite robust or that the finetuning didn\\'t drastically change performance. The omission of other dataset results due to no benefit from contamination might indicate that the main findings are from the clean datasets.\\n\\nI think the key points are the performance improvements with larger models, the minimal impact of dataset contamination, and the specific training parameters used. The tables provide detailed metrics, but without knowing exactly what each column represents, it\\'s hard to be more specific. However, the overall trend seems to be that larger models perform better, as the numbers increase with size.\\n\\nSo, the answer should summarize these points: varying model sizes, performance metrics across benchmarks, minimal impact from contamination, and the training setup.\\n</think>\\n\\nThe key findings from the llama2 finetuning benchmark experiments can be summarized as follows:\\n\\n1. **Performance Across Model Sizes**: The experiments evaluated models of varying sizes (7B, 13B, 34B, 70B) across multiple academic benchmarks. The results indicate that larger models generally perform better, as metrics improve with model size.\\n\\n2. **Minimal Impact of Dataset Contamination**: The analysis showed a small delta (-0.9) between clean subsets and sampling means, suggesting that dataset contamination had a limited impact on performance. Other datasets did not benefit from contamination, leading to their omission from the results.\\n\\n3. **Training Setup**: The models were trained using the AdamW optimizer with specific hyperparameters, including a cosine learning rate schedule, weight decay, and gradient clipping. These settings were effective, as evidenced by the training loss curves.\\n\\n4. **Benchmark Performance**: The models demonstrated strong performance across grouped academic benchmarks, with detailed results available for individual benchmarks. Safety benchmarks were noted but are discussed in a separate section.\\n\\nOverall, the experiments highlight the effectiveness of larger models and the robustness of the training approach, with minimal impact from dataset contamination.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What are the key findings from the llama2 finetuning benchmark experiments?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73d91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36323b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb9ba19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
